Authors: [[Jason Wei]], [[Xuezhi Wang]], [[Dale Schuurmans]], [[Maarten Bosma]], [[Brian Ichter]], [[Fei Xia]], [[Ed H. Chi]], [[Quoc V. Le]], [[Denny Zhou]]
Source: [[Google Research Brain Team]]
Link: https://arxiv.org/pdf/2201.11903

1. The paper explores how chain of thought prompting (A sequential set of reasoning steps) improves the ability of large language models to perform complex reasoning 
2. Traditional reasoning process are insufficient and cause LLM's to hallucinate
**Chain of Thought** - a series of intermediate natural language reasoning steps that lead to the final output.