[[Mrinank Sharma]]*+ [[Meg Tong]]* [[Jesse Mu]]* [[Jerry Wei]]* [[Jorrit Kruthoff]]* [[Scott Goodfriend]]* [[Euan Ong]]* [[Alwin Peng]]
**[[Anthropic]]**
https://arxiv.org/pdf/2501.18837

1. Big picture 
	1. LLMs are susceptible to universal jailbreaks that can cause them to provide responses that bypass their safeguards. This research aimed to show that defending against jailbreaks while maintaining practical deployment viability is possible.
	2. As LLMs are given more capability its important to ensure their ability to operate within the safeguards set for them
2. Approach
	1. Using synthetic data generated by "helpful only" LLMs, classifiers learned nuanced decisions boundaries. The data was augmented through paraphrasing translations prompt variations and compositional techniques.
	2. Through automated Red Teaming LLMs used knowledge of jailbreak techniques to autonomously generates new attack prompts 
	3. Assumptions:
		1. Synthetic data can adequately substitute for real attacks
		2. Step evidence - authors assume that success in each step needed for a harmful process is statistically independent and identically distributed, while this makes quantitative modeling easier its an abstraction from most real-world scenarios