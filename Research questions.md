
> How can long-term memory and context persistence in AI agents be aligned to prevent instruction drift over extended multi-step interactions?
> 
> What new defense mechanisms can be introduced to agent architectures to mitigate the risks introduced by multi-step and event-driven action generation?


How can we assure that the constitutional classifiers used to form the safeguards that "helpful-only" are trained with are adequate or evolve as LLMs and AI agents evolve and gain more capabilities

  

How can we design evaluation metrics or benchmarks that accurately capture both the helpfulness and harmlessness of aligned language models?

  

How might these metrics adapt as models increase in capability and as attack strategies evolve?

  

("Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis
" mentioned that apart of the reason why Ai agents are more suseptible may have to do the evaluation metrics used to measure the effectiveness of LLMs ability to remain harmless even with attempts of red teaming)

research project idea:
- adaptive (**context-aware**) safety guidelines for web connected AI agents
- benchmark how models can perform general retrieval tasks in chat environments
		- how to simulate the environment
		- deep research agent
- tool use



group meet recap:
workshop papers (our goal)
main track - more prestigious

